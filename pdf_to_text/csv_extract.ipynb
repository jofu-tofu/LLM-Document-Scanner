{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context of this Notebook\n",
    "Define a RAG workflow for generating our CSV from a query using LangGraph.\n",
    "\n",
    "#### Design\n",
    "\n",
    "The purpose is to take our pdf and generate a resulting csv with as much accuracy towards our intended purpose.\n",
    "The overarching framework will be defined by LangGraph. Within the LangGraph, we need a couple things.\n",
    "\n",
    "**State:** As the workflow is running, a common state object will be passed around describing the status quo of the LangGraph.\n",
    "\n",
    "**Agents:** The workflow will be run by several agents. In context of what we are doing, there are several that we might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document we are working with\n",
    "file_name = \"sample-new-fidelity-acnt-stmt\"\n",
    "local_llm = \"llama3\" # llama3/llama3.1:70b for 8b\n",
    "initial_question = \"Create CSV of asset holdings.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR on PDF and gives us text file\n",
    "%run -i pdf_to_text.py $file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Advisers, Inc. (SAI) are carried by NFS and covered by SIPC but do not contribute to your margin and maintenance requirements.  Short Account Balances Securities sold short are held in a segregated short account. These securities are marked-to-market for margin purposes, and any increase or decrease from the previous week's value is transferred weekly to your margin account. Fidelity represents your short account balance as of the last weekly mark-to-market, not as of the statement end date.  Information About Your Option Transactions Each transaction confirmation previously delivered to you contains full information about commissions and other charges, and such information is available promptly upon request. Assignments of American and European-style options are allocated among customer short positions pursuant to a random allocation procedure, a description is available upon request. Short positions in American-style options are liable for assignment anytime. The writer of a European-style option is\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RETREIVER DEFINITION\n",
    "import os\n",
    "from langchain_community.vectorstores import faiss\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# if not os.path.exists(f\"{file_name}\"):\n",
    "#     os.makedirs(\"text_data\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "embedding = NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\")\n",
    "os.makedirs(\"vectorstore\", exist_ok=True)\n",
    "if not os.path.exists(f\"vectorstore/{file_name}.faiss\"):\n",
    "    with open(f\"text_data/{file_name}.txt\", \"r\") as f:\n",
    "        pdf_text = f.read()\n",
    "        passages = text_splitter.split_text(pdf_text)\n",
    "        # Define open source embeddings\n",
    "        vectorstore = faiss.FAISS.from_texts(\n",
    "            texts=passages,\n",
    "            embedding=embedding,\n",
    "        )\n",
    "        vectorstore.save_local(\"vectorstore\",file_name)\n",
    "else:\n",
    "    vectorstore = faiss.FAISS.load_local(\"vectorstore\",embedding, file_name, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "retriever.invoke(\"Quantity of stocks and securities for the purpose of underwriting.\")[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT STATE DEFINITION\n",
    "\n",
    "import langgraph\n",
    "\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    hypodoc: Annotated[str, add_messages]\n",
    "    hypoCritics: Annotated[Sequence[langgraph.HypothesisDocument], add_messages]\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPO-DOC-CREATOR\n",
    "def hypo_doc_creator(state):\n",
    "    criticism = \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a hypothetical document creator that creates a hypotheical answer for the user's question. \\n \n",
    "        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "    hypo_doc = retriever.invoke(question)\n",
    "    return hypo_doc\n",
    "\n",
    "\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "### RETRIEVAL GRADER (MINIMUM TEST)\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def retrieval(state) -> Sequence[langgraph.Document]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    state[\"documents\"] = retriever.invoke(last_message.text)\n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Important dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('CUDA Available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"sample-new-fidelity-acnt-stmt\"\n",
    "pdf_data_path = \"./pdf_data/\" + file_name + \".pdf\"\n",
    "text_data_path = \"./text_data/\" + file_name + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,  we want to convert our PDF to image and run it through an OCR model. I will use Microsoft TrOCR tuned on invoice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\n",
    "ocr = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for image in images:\n",
    "        pil_image = image.convert('RGB')\n",
    "        pixel_values = processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = ocr.generate(pixel_values)\n",
    "        text += processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdf_text = ''\n",
    "os.makedirs(\"text_data\", exist_ok=True)\n",
    "if not os.path.exists(f\"text_data/{file_name}.txt\"):\n",
    "    with open(f\"text_data/{file_name}.txt\", \"w\") as f:\n",
    "        pdf_text = extract_text_from_pdf(pdf_data_path)\n",
    "        f.write(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our text-file, we want to transform our query into something better based on the info in the document. To do this, we need to first set up a retreiver strategy so we can get relevant information from the document we just created.\n",
    "\n",
    "Lets split our document into chunks. There are many ways to do this depending on needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEL:TEL :TOTALTOTALTOTALTOTALTOTAL :TOTAL :TOTAL :ITEMTOTAL :TOTAL:TOTAL:******TOTAL***TELTOTALTOTAL :TOTAL :TOTAL :***TOTALITEMITEM***']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "with open(f\"text_data/{file_name}.txt\", \"r\") as f:\n",
    "    pdf_text = f.read()\n",
    "passages = text_splitter.split_text(pdf_text)\n",
    "passages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to turn our passages into vector embeddings that we can store. This will help with retreiving relevant passages by putting them in a way that our machines can understand, a vector representation. We can use many different embeddings that depend on the type of documents we want to retreive from and what our purpose is. Different semantical meanings will have different embeddings depending on what you use.\n",
    "\n",
    "Nomic is a local lightweight embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "# Define open source embeddings\n",
    "embedding = NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\")\n",
    "\n",
    "vectorstore = SKLearnVectorStore.from_texts(\n",
    "    texts=passages,\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a vectorstore of our document. We set k=8 for the retreiver, but this can depend on our chunking and context length for our LLM model. We can also put in documents that augment this if needed. For example, a description may be important if a use queries on use case.\n",
    "\n",
    "Next step is to set up a RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'de869e23-3ae9-4cee-8830-447eae8b9c80'}, page_content='ee a 58 $s\\n** SAMPLE STATEMENT **\\n—— For informational purposes only veer went ReeORT\\n@ Fidelity So ecyoe an\\n** SAMPLE STATEMENT **\\n—— For informational purposes only nvesrient eePoRT\\n@ Fidelity So ecyoe an\\nHoldings (ontines ‘aocount 111411111\\nSh ne eae ayetiy cme 8 aie ior mere ence\\nErgon ramen tmesnmymmcrmmmeapiy, roms lpm tmtonme ence a\\nenMtemmmtreimeninmcmmence —\" amremas nnn sonare wmraeetaree\\nBema oemociasecmramatey — Rusmaniltracgee eae Grcerennnetnte'),\n",
       " Document(metadata={'id': 'c64ffb9f-aedf-4718-8ed3-05f27d4ed627'}, page_content='—— For informational purposes only nvesrient eePoRT\\n@ Fidelity So ecyoe an\\nHoldings (continued) pecount WTA 11111\\nsets\\nseston ‘ust een untae Costas ‘Smitece _neame yes Yen\\nSe CT\\nTod FrefsredSiock GM cfaccouthaares) Oar\\nsesaiptinn surty __uantty__rsrtnn_Aecwlvinmasiyay Cost ae ‘Goes _neoneeay tse\\n** SAMPLE STATEMENT **\\n—— For informational purposes only nvesrient eePoRT\\n@ Fidelity So ecyoe an\\nHoldings (continued) Recount WANNA\\nsesaiptinn sur use rortnt Atavasinnoanigy Costas ‘Goes _neoneeay tse\\nee a 58 $s'),\n",
       " Document(metadata={'id': '36ee0007-938d-44c4-8da8-679c75a0a5ee'}, page_content='@ Fidelity So ecyoe an\\nActivity continued) ‘aocount 111-111111\\n** SAMPLE STATEMENT **\\n—— For informational purposes only nvesrient eePoRT\\n@ Fidelity So ecyoe an\\nActivity continued) ‘aocount 111411111\\nellen, een\\nrevi revi enance \"hate __ aly asn’s__ntrea Patt arog revi enaace \"Rate __aty asan’s__ntrot rat\\n** SAMPLE STATEMENT **'),\n",
       " Document(metadata={'id': 'e5145b96-9227-4f9a-8f06-3e2d8b3d112f'}, page_content='Esimated Cash Fow socount zncz0202\\nsom and £60 incame, and & C0 Principal, sock name, sus Fund income curren\\ngare\" st\\nesenoe co = = = Es\\novens = = = = =\\nSee\\nvay = = = =\\nBay Ba\\nTera rr = ie = co\\ntend 6D neame ncuden see gaynesfrtred ne tonaou! Cenc Cont Com\\nThelebe nove doesnotincrncesn Bow om ie otewng scutes poe? ad amet Sos. xchange ode profuds(EPPs LEPNS) Us. ante\\nskins androids ye ncaa oracana\\n** SAMPLE STATEMENT **\\nFor informational purposes only uvesritenr nevonr\\nFidelity Shy uy $1208')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Get me a csv of all asset quantities.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
